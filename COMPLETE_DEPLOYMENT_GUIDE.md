# Complete Auto Intel Project Deployment Guide
## Docker + Airflow Orchestration

---

## ğŸš€ **Project Overview**

The Auto Intel project is a comprehensive automotive data intelligence platform that integrates:
- **Web Scraping** (Scrapy spiders)
- **AI Analysis** (NLP, sentiment analysis, correlations)
- **Interactive Dashboard** (Streamlit)
- **REST API** (FastAPI)
- **Orchestration** (Apache Airflow)
- **Data Storage** (PostgreSQL + Redis)

---

## ğŸ“‹ **Prerequisites**

### **System Requirements**
- **OS:** Windows 10/11, macOS, or Linux
- **RAM:** Minimum 8GB (16GB recommended)
- **Storage:** 20GB free space
- **CPU:** 4 cores minimum

### **Software Requirements**
- **Docker Desktop** (latest version)
- **Git** (for cloning the repository)
- **Text Editor** (VS Code, Sublime, etc.)

### **Install Docker Desktop**
1. Download from [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)
2. Install and restart your computer
3. Verify installation: `docker --version` and `docker-compose --version`

---

## ğŸ—ï¸ **Project Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Sources   â”‚    â”‚  Auto Intel     â”‚    â”‚   PostgreSQL    â”‚
â”‚   (News/Reviews)â”‚â”€â”€â”€â–¶â”‚   Scrapy        â”‚â”€â”€â”€â–¶â”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Spiders       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                â”‚                      â”‚
                                â–¼                      â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Airflow       â”‚    â”‚   Redis Cache   â”‚
                       â”‚   Orchestration â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   AI Analysis   â”‚    â”‚   FastAPI       â”‚
                       â”‚   Engine        â”‚â”€â”€â”€â–¶â”‚   REST API      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                      â”‚
                                â–¼                      â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Streamlit     â”‚    â”‚   End Users     â”‚
                       â”‚   Dashboard     â”‚â—€â”€â”€â”€â”‚   (Web/Mobile)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ **Project Structure**

```
auto_intel_project/
â”œâ”€â”€ auto_intel/              # Scrapy spiders and pipelines
â”œâ”€â”€ analysis/                # AI analysis modules
â”œâ”€â”€ api/                     # FastAPI application
â”œâ”€â”€ dashboard/               # Streamlit dashboard
â”œâ”€â”€ airflow/                 # Airflow configuration
â”‚   â”œâ”€â”€ dags/               # Airflow DAGs
â”‚   â”œâ”€â”€ logs/               # Airflow logs
â”‚   â””â”€â”€ config/             # Airflow config
â”œâ”€â”€ project_data/            # CSV datasets
â”œâ”€â”€ tests/                   # Unit and integration tests
â”œâ”€â”€ docker-compose.yaml      # Main deployment file
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ³ **Step-by-Step Deployment**

### **Step 1: Clone and Setup Project**

```bash
# Clone the repository
git clone <your-repo-url>
cd auto_intel_project

# Create necessary directories
mkdir -p airflow/dags airflow/logs airflow/config airflow/plugins
mkdir -p logs data project_data
```

### **Step 2: Environment Configuration**

Create a `.env` file in the project root:

```bash
# Create .env file
cat > .env << EOF
# Airflow Configuration
AIRFLOW_UID=50000
AIRFLOW_IMAGE_NAME=apache/airflow:3.0.3
AIRFLOW_PROJ_DIR=.

# Airflow Admin User
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin123

# Database Configuration
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# Auto Intel Configuration
DATABASE_URL=postgresql://airflow:airflow@postgres:5432/airflow
REDIS_URL=redis://redis:6379/0
EOF
```

### **Step 3: Build and Start Services**

```bash
# Build all Docker images
docker-compose build

# Start all services
docker-compose up -d

# Check service status
docker-compose ps
```

### **Step 4: Verify Service Health**

```bash
# Check all services are running
docker-compose ps

# Check service logs
docker-compose logs -f

# Check specific service logs
docker-compose logs -f auto-intel-api
docker-compose logs -f airflow-scheduler
```

---

## ğŸ”§ **Service Configuration Details**

### **1. Auto Intel Scrapy Service**
- **Port:** Internal only
- **Purpose:** Web scraping automotive data
- **Dependencies:** PostgreSQL
- **Health Check:** Internal monitoring

### **2. Auto Intel API Service**
- **Port:** 8000
- **Purpose:** REST API for data access
- **Dependencies:** PostgreSQL, Redis
- **Health Check:** HTTP endpoint `/health`

### **3. Auto Intel Dashboard Service**
- **Port:** 8501
- **Purpose:** Interactive data visualization
- **Dependencies:** PostgreSQL, Redis, API
- **Health Check:** Streamlit health endpoint

### **4. PostgreSQL Database**
- **Port:** 5432 (internal)
- **Purpose:** Primary data storage
- **Dependencies:** None
- **Health Check:** Database connectivity

### **5. Redis Cache**
- **Port:** 6379 (internal)
- **Purpose:** Caching and task queue
- **Dependencies:** None
- **Health Check:** Redis ping

### **6. Airflow Services**
- **Web UI:** Port 8080
- **Scheduler:** Internal
- **Worker:** Internal
- **DAG Processor:** Internal
- **Triggerer:** Internal

---

## ğŸš€ **Airflow Orchestration Setup**

### **Step 1: Access Airflow Web UI**

1. Open browser and go to: `http://localhost:8080`
2. Login with credentials:
   - **Username:** `admin`
   - **Password:** `admin123`

### **Step 2: Create Auto Intel DAG**

Create file `airflow/dags/auto_intel_pipeline.py`:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.dummy_operator import DummyOperator

default_args = {
    'owner': 'auto_intel',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'auto_intel_pipeline',
    default_args=default_args,
    description='Complete Auto Intel data pipeline',
    schedule_interval=timedelta(hours=6),
    catchup=False,
    tags=['auto_intel', 'automotive', 'data_intelligence'],
)

# Start task
start = DummyOperator(task_id='start', dag=dag)

# Data collection task
collect_data = BashOperator(
    task_id='collect_data',
    bash_command='cd /opt/airflow/projects/auto_intel_project && python -m scrapy crawl auto_news && python -m scrapy crawl auto_reviews',
    dag=dag
)

# Data processing task
process_data = PythonOperator(
    task_id='process_data',
    python_callable=lambda: print("Data processing completed"),
    dag=dag
)

# AI analysis task
run_analysis = PythonOperator(
    task_id='run_analysis',
    python_callable=lambda: print("AI analysis completed"),
    dag=dag
)

# API health check task
check_api = BashOperator(
    task_id='check_api',
    bash_command='curl -f http://auto-intel-api:8000/health || exit 1',
    dag=dag
)

# Dashboard health check task
check_dashboard = BashOperator(
    task_id='check_dashboard',
    bash_command='curl -f http://auto-intel-dashboard:8501/_stcore/health || exit 1',
    dag=dag
)

# End task
end = DummyOperator(task_id='end', dag=dag)

# Define task dependencies
start >> collect_data >> process_data >> run_analysis >> [check_api, check_dashboard] >> end
```

### **Step 3: Monitor DAG Execution**

1. **View DAGs:** Go to DAGs tab in Airflow UI
2. **Trigger DAG:** Click "Play" button to run manually
3. **Monitor Progress:** View real-time execution in Graph view
4. **Check Logs:** Click on individual tasks to view logs

---

## ğŸ“Š **Accessing Services**

### **1. Streamlit Dashboard**
- **URL:** `http://localhost:8501`
- **Features:** 6 interactive pages with automotive insights
- **Real-time:** Live data updates and visualizations

### **2. FastAPI Documentation**
- **URL:** `http://localhost:8000/docs`
- **Features:** Interactive API documentation
- **Endpoints:** 8+ REST API endpoints

### **3. Airflow Web UI**
- **URL:** `http://localhost:8080`
- **Features:** Pipeline orchestration and monitoring
- **Credentials:** admin/admin123

### **4. Flower (Celery Monitoring)**
- **URL:** `http://localhost:5555`
- **Features:** Task queue monitoring
- **Access:** `docker-compose --profile flower up`

---

## ğŸ” **Monitoring and Health Checks**

### **Service Health Monitoring**

```bash
# Check all service health
docker-compose ps

# Check specific service health
docker-compose exec auto-intel-api curl -f http://localhost:8000/health
docker-compose exec auto-intel-dashboard curl -f http://localhost:8501/_stcore/health

# Check database connectivity
docker-compose exec postgres pg_isready -U airflow

# Check Redis connectivity
docker-compose exec redis redis-cli ping
```

### **Log Monitoring**

```bash
# View all service logs
docker-compose logs -f

# View specific service logs
docker-compose logs -f auto-intel-api
docker-compose logs -f airflow-scheduler
docker-compose logs -f auto-intel-dashboard

# Follow logs in real-time
docker-compose logs -f --tail=100
```

### **Performance Monitoring**

```bash
# Check resource usage
docker stats

# Check disk usage
docker system df

# Check network usage
docker network ls
docker network inspect auto_intel_project_auto-intel-network
```

---

## ğŸ› ï¸ **Troubleshooting Common Issues**

### **Issue 1: Services Not Starting**

```bash
# Check service status
docker-compose ps

# Check service logs
docker-compose logs <service-name>

# Restart specific service
docker-compose restart <service-name>

# Restart all services
docker-compose down
docker-compose up -d
```

### **Issue 2: Database Connection Errors**

```bash
# Check PostgreSQL status
docker-compose exec postgres pg_isready -U airflow

# Check database logs
docker-compose logs postgres

# Restart database
docker-compose restart postgres
```

### **Issue 3: Airflow DAGs Not Running**

```bash
# Check Airflow scheduler
docker-compose logs airflow-scheduler

# Check DAG processor
docker-compose logs airflow-dag-processor

# Restart Airflow services
docker-compose restart airflow-scheduler airflow-dag-processor
```

### **Issue 4: Port Conflicts**

```bash
# Check port usage
netstat -tulpn | grep :8000
netstat -tulpn | grep :8501
netstat -tulpn | grep :8080

# Change ports in docker-compose.yaml if needed
ports:
  - "8001:8000"  # Change external port
```

---

## ğŸ“ˆ **Scaling and Performance**

### **Horizontal Scaling**

```bash
# Scale API service to 3 instances
docker-compose up -d --scale auto-intel-api=3

# Scale dashboard service to 2 instances
docker-compose up -d --scale auto-intel-dashboard=2

# Scale Airflow workers to 3 instances
docker-compose up -d --scale airflow-worker=3
```

### **Load Balancing**

```bash
# Add Nginx load balancer
docker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d
```

### **Resource Limits**

```bash
# Set resource limits in docker-compose.yaml
services:
  auto-intel-api:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
```

---

## ğŸ”’ **Security Configuration**

### **Environment Variables**

```bash
# Create production .env file
cat > .env.prod << EOF
# Production Environment
AIRFLOW_UID=50000
AIRFLOW_IMAGE_NAME=apache/airflow:3.0.3

# Secure Credentials
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=<secure-password>
POSTGRES_PASSWORD=<secure-password>
REDIS_PASSWORD=<secure-password>

# SSL Configuration
ENABLE_SSL=true
SSL_CERT_PATH=/path/to/cert.pem
SSL_KEY_PATH=/path/to/key.pem
EOF
```

### **Network Security**

```bash
# Create secure network
docker network create --driver bridge --subnet=172.20.0.0/16 auto-intel-secure

# Update docker-compose.yaml to use secure network
networks:
  auto-intel-secure:
    external: true
```

---

## ğŸ“š **Maintenance and Updates**

### **Regular Maintenance**

```bash
# Update Docker images
docker-compose pull
docker-compose up -d

# Clean up unused resources
docker system prune -f
docker volume prune -f

# Backup database
docker-compose exec postgres pg_dump -U airflow airflow > backup_$(date +%Y%m%d_%H%M%S).sql
```

### **Log Rotation**

```bash
# Configure log rotation in docker-compose.yaml
services:
  auto-intel-api:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

### **Performance Optimization**

```bash
# Monitor performance
docker stats --no-stream

# Optimize database queries
docker-compose exec postgres psql -U airflow -d airflow -c "ANALYZE;"

# Clear Redis cache if needed
docker-compose exec redis redis-cli FLUSHALL
```

---

## ğŸ¯ **Production Deployment Checklist**

### **Pre-Deployment**
- [ ] All services tested locally
- [ ] Environment variables configured
- [ ] SSL certificates ready
- [ ] Database backup strategy in place
- [ ] Monitoring tools configured

### **Deployment**
- [ ] Production environment prepared
- [ ] Docker images built and tested
- [ ] Services deployed with health checks
- [ ] Load balancer configured
- [ ] SSL certificates deployed

### **Post-Deployment**
- [ ] All services responding correctly
- [ ] Airflow DAGs running successfully
- [ ] Performance metrics within acceptable ranges
- [ ] Error monitoring active
- [ ] Backup systems tested

---

## ğŸ“ **Support and Resources**

### **Useful Commands**

```bash
# Quick status check
docker-compose ps

# View all logs
docker-compose logs -f

# Restart everything
docker-compose down && docker-compose up -d

# Check resource usage
docker stats

# Access service shell
docker-compose exec auto-intel-api bash
docker-compose exec airflow-scheduler bash
```

### **Documentation Links**
- **Docker Compose:** [https://docs.docker.com/compose/](https://docs.docker.com/compose/)
- **Apache Airflow:** [https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)
- **FastAPI:** [https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/)
- **Streamlit:** [https://docs.streamlit.io/](https://docs.streamlit.io/)

---

## ğŸ‰ **Deployment Complete!**

Your Auto Intel project is now fully deployed with:
- âœ… **Web Scraping** - Automated data collection
- âœ… **AI Analysis** - NLP and correlation analysis
- âœ… **Interactive Dashboard** - Real-time visualizations
- âœ… **REST API** - Programmatic data access
- âœ… **Airflow Orchestration** - Automated pipeline management
- âœ… **Production Ready** - Scalable and monitored

**Access Points:**
- **Dashboard:** http://localhost:8501
- **API Docs:** http://localhost:8000/docs
- **Airflow UI:** http://localhost:8080 (admin/admin123)

**Next Steps:**
1. Monitor Airflow DAGs for successful execution
2. Test API endpoints for data access
3. Explore dashboard for insights
4. Configure additional DAGs as needed
5. Set up monitoring and alerting

---

*This deployment guide provides comprehensive instructions for deploying the complete Auto Intel project with Docker and Airflow orchestration. Follow each step carefully to ensure successful deployment.*
